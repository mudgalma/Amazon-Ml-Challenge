{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "DVR44EUdW__3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('combined.csv')\n",
        "test = pd.read_csv('test_data.csv')\n",
        "df = pd.read_csv('filtered_data.csv')"
      ],
      "metadata": {
        "id": "MsreJiAiXBSc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape, df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJt3rf8Zo9XN",
        "outputId": "5bd22edd-49cd-4cdc-cc29-4c7fb4a00175"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 6), (131187, 6), (41953, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_entity_unit_map = {\n",
        "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
        "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
        "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
        "    'wattage': {'kilowatt', 'watt'},\n",
        "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
        "}\n",
        "\n",
        "entity_unit_map = {\n",
        "    'width': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
        "    'depth': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
        "    'height': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
        "    'item_weight': {'gram', 'gm', 'g', 'kilogram', 'kg', 'microgram', 'µg', 'milligram', 'mg', 'ounce', 'oz', 'pound', 'lb', 'ton', 't'},\n",
        "    'maximum_weight_recommendation': {'gram', 'g', 'gm', 'kilogram', 'kg', 'microgram', 'µg', 'milligram', 'mg', 'ounce', 'oz', 'pound', 'lb', 'ton', 't'},\n",
        "    'voltage': {'kilovolt', 'kv', 'millivolt', 'mv', 'volt', 'v'},\n",
        "    'wattage': {'kilowatt', 'kw', 'watt', 'w'},\n",
        "    'item_volume': {'centilitre', 'cl', 'cubic foot', 'ft³', 'cubic inch', 'in³', 'cup', 'decilitre', 'dl', 'fluid ounce', 'fl oz', 'gallon', 'gal', 'imperial gallon', 'imp gal', 'litre', 'l', 'microlitre', 'µl', 'millilitre', 'ml', 'pint', 'pt', 'quart', 'qt'}\n",
        "}\n",
        "\n",
        "unit_corrections = {\n",
        "    'cm': 'centimetre',\n",
        "    'm': 'metre',\n",
        "    'mm': 'millimetre',\n",
        "    'ft': 'foot',\n",
        "    'in': 'inch',\n",
        "    'yd': 'yard',\n",
        "    'g': 'gram',\n",
        "    'kg': 'kilogram',\n",
        "    'lb': 'pound',\n",
        "    'oz': 'ounce',\n",
        "    'mg': 'milligram',\n",
        "    'µg': 'microgram',\n",
        "    't': 'ton',\n",
        "    'kv': 'kilovolt',\n",
        "    'mv': 'millivolt',\n",
        "    'v': 'volt',\n",
        "    'w': 'watt',\n",
        "    'kw': 'kilowatt',\n",
        "    'l': 'litre',\n",
        "    'ml': 'millilitre',\n",
        "    'µl': 'microlitre',\n",
        "    'cl': 'centilitre',\n",
        "    'dl': 'decilitre',\n",
        "    'fl oz': 'fluid ounce',\n",
        "    'pt': 'pint',\n",
        "    'qt': 'quart',\n",
        "    'gal': 'gallon',\n",
        "    'imp gal': 'imperial gallon',\n",
        "    'ft³': 'cubic foot',\n",
        "    'in³': 'cubic inch'\n",
        "}"
      ],
      "metadata": {
        "id": "IlCKxkW1q7GC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_numeric_value_with_unit(sentence, entity_name):\n",
        "    # Define priorities for certain keywords\n",
        "    priority_keywords = ['net weight', 'net wt','netwt','weight', 'weight net']\n",
        "\n",
        "    # Get the units associated with the entity\n",
        "    units = entity_unit_map.get(entity_name, set())\n",
        "    valid_units = valid_entity_unit_map.get(entity_name, set())\n",
        "\n",
        "    # Create a regex pattern to match numeric values followed by a unit\n",
        "    units_pattern = '|'.join(re.escape(unit) for unit in units)\n",
        "    regex_pattern = rf'(\\d+(?:[.,]\\d+)?)\\s*({units_pattern})'\n",
        "\n",
        "    # Find all matches in the sentence\n",
        "    matches = re.findall(regex_pattern, sentence, re.IGNORECASE)\n",
        "\n",
        "    # Check for priority keywords and extract values accordingly\n",
        "    extracted_values = []\n",
        "    if any(keyword in sentence.lower() for keyword in priority_keywords):\n",
        "        # If priority keywords are found, focus on extracting those values\n",
        "        for match in matches:\n",
        "            numeric_value = match[0].replace(',', '.').strip()  # Replace comma with dot for decimal\n",
        "            unit = match[1].lower()  # Ensure case insensitivity\n",
        "\n",
        "            # Replace unit with the valid form if a correction exists\n",
        "            standardized_unit = unit_corrections.get(unit, unit)\n",
        "\n",
        "            # Check if the standardized unit is valid for the given entity\n",
        "            if standardized_unit in valid_units:\n",
        "                extracted_values.append((float(numeric_value), standardized_unit))\n",
        "\n",
        "        # If any valid value was found, return the one with the highest numeric value\n",
        "        if extracted_values:\n",
        "            max_value, best_unit = max(extracted_values, key=lambda x: x[0])\n",
        "            return f\"{max_value} {best_unit}\"\n",
        "\n",
        "    # If no priority keywords were found, return the highest numeric value from all matches\n",
        "    else:\n",
        "        for match in matches:\n",
        "            numeric_value = match[0].replace(',', '.').strip()  # Replace comma with dot for decimal\n",
        "            unit = match[1].lower()  # Ensure case insensitivity\n",
        "\n",
        "            # Replace unit with the valid form if a correction exists\n",
        "            standardized_unit = unit_corrections.get(unit, unit)\n",
        "\n",
        "            # Check if the standardized unit is valid for the given entity\n",
        "            if standardized_unit in valid_units:\n",
        "                extracted_values.append((float(numeric_value), standardized_unit))\n",
        "\n",
        "        # If any valid value was found, return the one with the highest numeric value\n",
        "        if extracted_values:\n",
        "            max_value, best_unit = max(extracted_values, key=lambda x: x[0])\n",
        "            return f\"{max_value} {best_unit}\"\n",
        "\n",
        "    return ''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nnX1QAxXjcA9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tRvXkjq3CVi2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_numeric_value_with_unit(sentence, entity_name):\n",
        "#     # Get the units associated with the entity\n",
        "#     units = entity_unit_map.get(entity_name, set())\n",
        "#     valid_units = valid_entity_unit_map.get(entity_name, set())\n",
        "\n",
        "#     # Create a regex pattern to match numeric values followed by a unit\n",
        "#     units_pattern = '|'.join(re.escape(unit) for unit in units)\n",
        "#     regex_pattern = rf'(\\d+(?:\\.\\d+)?)\\s*({units_pattern})'\n",
        "\n",
        "#     # Find all matches in the sentence\n",
        "#     matches = re.findall(regex_pattern, sentence, re.IGNORECASE)\n",
        "\n",
        "#     if matches:\n",
        "#         # Store all valid numeric values with units\n",
        "#         extracted_values = []\n",
        "\n",
        "#         for match in matches:\n",
        "#             numeric_value = match[0]\n",
        "#             unit = match[1].lower()  # Ensure case insensitivity\n",
        "\n",
        "#             # Replace unit with the valid form if a correction exists\n",
        "#             standardized_unit = unit_corrections.get(unit, unit)\n",
        "\n",
        "#             # Check if the standardized unit is valid for the given entity\n",
        "#             if standardized_unit in valid_units:\n",
        "#                 extracted_values.append((float(numeric_value), standardized_unit))\n",
        "\n",
        "#         # If any valid value was found, return the one with the highest numeric value\n",
        "#         if extracted_values:\n",
        "#             max_value, best_unit = max(extracted_values, key=lambda x: x[0])\n",
        "#             return f\"{max_value} {best_unit}\"\n",
        "\n",
        "#     return ''"
      ],
      "metadata": {
        "id": "cI7SPhs5tuB9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['prediction'] = test.apply(\n",
        "    lambda row: extract_numeric_value_with_unit(\n",
        "        str(row['paddleocr']) if pd.notnull(row['paddleocr']) else '',\n",
        "        str(row['entity_name']) if pd.notnull(row['entity_name']) else ''\n",
        "    ),\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "O7tiS0bFeU8g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['paddleocr'].fillna('constant_value', inplace=True)"
      ],
      "metadata": {
        "id": "aZKCUGgQ0Dvx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=test"
      ],
      "metadata": {
        "id": "wmUYUzABviy9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMeE6WhJyHst",
        "outputId": "657e737b-5d08-403c-ffdb-4894eaa9fedc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Updated preprocessing function to apply lowercase only to alphabetic characters\n",
        "def preprocess_text(text):\n",
        "    # Ensure the input is a string\n",
        "    if isinstance(text, str):\n",
        "        # Apply lowercase only to alphabetic characters, leaving digits unchanged\n",
        "        text = re.sub(r'[a-zA-Z]+', lambda match: match.group(0).lower(), text)\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        # Remove non-alphabetic and non-digit characters\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        # Tokenize text\n",
        "        tokens = word_tokenize(text)\n",
        "        # Remove stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        return tokens\n",
        "    else:\n",
        "        # Return an empty list for non-string values (e.g., NaN, None)\n",
        "        return []\n",
        "\n",
        "# Apply preprocessing to the 'extracted_text' column\n",
        "df['tokens'] = df['paddleocr'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "nINKLc9yv_bp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a preprocessing function\n",
        "\n",
        "\n",
        "# Train Word2Vec on the preprocessed tokenized text\n",
        "model = Word2Vec(df['tokens'], vector_size=30, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to get the average word vector for each sentence\n",
        "def get_avg_wordvec(tokens, model):\n",
        "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
        "    return sum(vectors) / len(vectors) if vectors else [0] * model.vector_size\n",
        "\n",
        "# Apply the function to create a text vector for each row\n",
        "df['text_vector'] = df['tokens'].apply(lambda x: get_avg_wordvec(x, model))\n",
        "\n",
        "# Convert the text_vector column (list of vectors) into multiple columns\n",
        "text_vector_df = pd.DataFrame(df['text_vector'].tolist(), index=df.index)\n",
        "\n",
        "# Concatenate the new DataFrame with the original DataFrame\n",
        "df = pd.concat([df, text_vector_df], axis=1)\n",
        "\n",
        "# Drop the 'text_vector' and 'tokens' columns if needed\n",
        "df = df.drop(columns=['text_vector', 'tokens'])\n",
        "\n",
        "# Now df is ready to be used for Random Forest or any other model\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-nHuUIYypAu",
        "outputId": "13534009-e394-4646-b4a6-8f24e1ec8db5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   index                                         image_link  group_id  \\\n",
            "0      0  https://m.media-amazon.com/images/I/110EibNycl...    156839   \n",
            "1      1  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
            "2      2  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
            "3      3  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
            "4      4  https://m.media-amazon.com/images/I/11gHj8dhhr...    792578   \n",
            "\n",
            "  entity_name         filename  \\\n",
            "0      height  110EibNyclL.jpg   \n",
            "1       width  11TU2clswzL.jpg   \n",
            "2      height  11TU2clswzL.jpg   \n",
            "3       depth  11TU2clswzL.jpg   \n",
            "4       depth  11gHj8dhhrL.jpg   \n",
            "\n",
            "                                           paddleocr         prediction  \\\n",
            "0       2.63in 6.68cm 91.44cm - 199.39cm 36in - 78in  199.39 centimetre   \n",
            "1  Size Width Length One Size 42cm/16.54\" 200cm/7...   200.0 centimetre   \n",
            "2  Size Width Length One Size 42cm/16.54\" 200cm/7...   200.0 centimetre   \n",
            "3  Size Width Length One Size 42cm/16.54\" 200cm/7...   200.0 centimetre   \n",
            "4  Size Width Length One Size 10.50cm/4.13\" 90cm/...    90.0 centimetre   \n",
            "\n",
            "          0         1         2  ...        20        21        22        23  \\\n",
            "0  0.230061 -0.462712 -0.058042  ...  0.161356 -0.473988  0.004289  0.286443   \n",
            "1 -0.317392 -1.891624  2.881192  ... -0.747540  0.264763 -1.310949  3.184422   \n",
            "2 -0.317392 -1.891624  2.881192  ... -0.747540  0.264763 -1.310949  3.184422   \n",
            "3 -0.317392 -1.891624  2.881192  ... -0.747540  0.264763 -1.310949  3.184422   \n",
            "4 -0.313776 -1.898181  2.884802  ... -0.732773  0.269036 -1.298962  3.187831   \n",
            "\n",
            "         24        25        26        27        28        29  \n",
            "0  0.398257  0.209992  0.045633  0.156599 -0.079648 -0.736994  \n",
            "1  1.323523 -0.602655 -0.663186 -0.554517  0.541351 -4.673419  \n",
            "2  1.323523 -0.602655 -0.663186 -0.554517  0.541351 -4.673419  \n",
            "3  1.323523 -0.602655 -0.663186 -0.554517  0.541351 -4.673419  \n",
            "4  1.324485 -0.596464 -0.660025 -0.541011  0.546444 -4.672191  \n",
            "\n",
            "[5 rows x 37 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('df_test')"
      ],
      "metadata": {
        "id": "7TxZ4JTQQ07n"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['image_link', 'filename'])"
      ],
      "metadata": {
        "id": "3lD67gOTDYni"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load the dataset\n",
        " # Update with your actual training data path\n",
        "\n",
        "# Function to separate numeric values and units\n",
        "def separate_numeric_and_units(text):\n",
        "    if isinstance(text, str):\n",
        "        # Use regular expressions to extract numeric values and units\n",
        "        numeric_value = re.findall(r'[\\d\\.]+', text)\n",
        "        unit = re.findall(r'[a-zA-Z]+', text)\n",
        "\n",
        "        # Handle missing or invalid values\n",
        "        numeric_value = numeric_value[0] if numeric_value else None\n",
        "        unit = unit[0] if unit else 'unknown'\n",
        "\n",
        "        return numeric_value, unit\n",
        "    else:\n",
        "        return None, 'unknown'\n",
        "\n",
        "# Apply separation to both entity_value and prediction columns\n",
        "df['entity_numeric'], df['entity_unit'] = zip(*df['entity_value'].apply(separate_numeric_and_units))\n",
        "df['prediction_numeric'], df['prediction_unit'] = zip(*df['prediction'].apply(separate_numeric_and_units))\n",
        "\n",
        "# Check the output to ensure values are correctly split\n",
        "print(df[['entity_numeric', 'entity_unit', 'prediction_numeric', 'prediction_unit']].head())\n",
        "\n",
        "\n",
        "# 2.1 Separate numeric values and units from `entity_value` and `prediction` columns\n",
        "\n",
        "# Replace 'missing' with 'unknown' for consistent encoding\n",
        "\n",
        "\n",
        "\n",
        "# 2.2 Encode categorical columns (units)\n",
        "encoder = LabelEncoder()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIYnJuriCb2_",
        "outputId": "6f59b70f-dfde-4c93-d549-d99ecf160de8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  entity_numeric entity_unit prediction_numeric prediction_unit\n",
            "0          500.0        gram              500.0            gram\n",
            "1            1.0         cup               None         unknown\n",
            "2          0.709        gram              200.0       milligram\n",
            "3          0.709        gram                1.0             ton\n",
            "4           1400   milligram               None         unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the 'entity_unit' column\n",
        "encoder.fit(df['entity_unit'])\n",
        "\n",
        "# Apply the transformation with a fallback for unseen labels\n",
        "# Use np.where to handle unknown labels manually\n",
        "df['entity_unit_encoded'] = encoder.transform(df['entity_unit'])\n",
        "\n",
        "# For 'prediction_unit', handle unseen labels by assigning a default value, like -1\n",
        "try:\n",
        "    df['prediction_unit_encoded'] = encoder.transform(df['prediction_unit'])\n",
        "except ValueError:\n",
        "    # If unseen labels are encountered, map them to a default value\n",
        "    df['prediction_unit_encoded'] = df['prediction_unit'].apply(\n",
        "        lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1\n",
        "    )\n",
        "\n",
        "# Check the transformation\n",
        "print(df[['entity_unit', 'entity_unit_encoded', 'prediction_unit', 'prediction_unit_encoded']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTBJJoL6KIXR",
        "outputId": "89f262c8-eee2-4895-b768-ef8f2ac2c581"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  entity_unit  entity_unit_encoded prediction_unit  prediction_unit_encoded\n",
            "0        gram                   12            gram                       12\n",
            "1         cup                    5         unknown                       -1\n",
            "2        gram                   12       milligram                       21\n",
            "3        gram                   12             ton                       30\n",
            "4   milligram                   21         unknown                       -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['entity_combined'] = df['entity_numeric'].astype(str) + \" \" + df['entity_unit']\n",
        "df['prediction_combined'] = df['prediction_numeric'].astype(str) + \" \" + df['prediction_unit']\n",
        "\n",
        "# Encode the combined entity value for classification\n",
        "df['entity_combined_encoded'] = encoder.fit_transform(df['entity_combined'].fillna('missing'))\n",
        "\n",
        "# 2.3 Apply MinMax scaling to numeric columns (except prediction feature)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "numeric_columns = ['entity_numeric', 'group_id']  # Add more columns if needed\n",
        "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
        "\n",
        "\n",
        "\n",
        "# Convert all column names to strings\n",
        "df.columns = df.columns.astype(str)\n",
        "\n",
        "df['entity_name_encoded'] = encoder.fit_transform(df['entity_name'])\n",
        "\n",
        "# Drop the original 'entity_name' column if it's no longer needed\n",
        "df = df.drop(columns=['entity_name'])\n",
        "\n",
        "# Proceed with further steps\n",
        "\n",
        "# 3. Train-test split\n",
        "X = df.drop(columns=['entity_value', 'prediction', 'entity_numeric', 'prediction_numeric', 'entity_combined', 'entity_combined_encoded'])\n",
        "X = df.select_dtypes(exclude=['object'])\n",
        "y = df['entity_combined_encoded']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a classification model (RandomForestClassifier)\n"
      ],
      "metadata": {
        "id": "MS13S5smKLdu"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4xv-qyfKjqI",
        "outputId": "c83a3cbc-a82e-4b4e-c10f-9de794360365"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 40000 entries, 39087 to 15795\n",
            "Data columns (total 36 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   group_id                 40000 non-null  float64\n",
            " 1   0                        40000 non-null  float64\n",
            " 2   1                        40000 non-null  float64\n",
            " 3   2                        40000 non-null  float64\n",
            " 4   3                        40000 non-null  float64\n",
            " 5   4                        40000 non-null  float64\n",
            " 6   5                        40000 non-null  float64\n",
            " 7   6                        40000 non-null  float64\n",
            " 8   7                        40000 non-null  float64\n",
            " 9   8                        40000 non-null  float64\n",
            " 10  9                        40000 non-null  float64\n",
            " 11  10                       40000 non-null  float64\n",
            " 12  11                       40000 non-null  float64\n",
            " 13  12                       40000 non-null  float64\n",
            " 14  13                       40000 non-null  float64\n",
            " 15  14                       40000 non-null  float64\n",
            " 16  15                       40000 non-null  float64\n",
            " 17  16                       40000 non-null  float64\n",
            " 18  17                       40000 non-null  float64\n",
            " 19  18                       40000 non-null  float64\n",
            " 20  19                       40000 non-null  float64\n",
            " 21  20                       40000 non-null  float64\n",
            " 22  21                       40000 non-null  float64\n",
            " 23  22                       40000 non-null  float64\n",
            " 24  23                       40000 non-null  float64\n",
            " 25  24                       40000 non-null  float64\n",
            " 26  25                       40000 non-null  float64\n",
            " 27  26                       40000 non-null  float64\n",
            " 28  27                       40000 non-null  float64\n",
            " 29  28                       40000 non-null  float64\n",
            " 30  29                       40000 non-null  float64\n",
            " 31  entity_numeric           40000 non-null  float64\n",
            " 32  entity_unit_encoded      40000 non-null  int64  \n",
            " 33  prediction_unit_encoded  40000 non-null  int64  \n",
            " 34  entity_combined_encoded  40000 non-null  int64  \n",
            " 35  entity_name_encoded      40000 non-null  int64  \n",
            "dtypes: float64(32), int64(4)\n",
            "memory usage: 11.3 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQ1gC5MkVz-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(train['entity_value'], train['prediction'], average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB7AZiS8ih-X",
        "outputId": "3c2b9a0a-ad50-415d-aa87-23b1eb9712cb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29396680366338546"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test['prediction'] = test.apply(\n",
        "    lambda row: extract_numeric_value_with_unit(\n",
        "        str(row['paddleocr']) if pd.notnull(row['paddleocr']) else '',\n",
        "        str(row['entity_name']) if pd.notnull(row['entity_name']) else ''\n",
        "    ),\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "0VvsApUHvI_r"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = test[['index', 'prediction']]\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "XeYgSBUk11FO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wE7ooPqU5YGd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}